COMPONENTS


The model itself is Ministral-3-14B-Instruct. 


European and pretty solid for the size. The reasoning version takes too long.


I used a version uncensored with the Absolute Heresy protocol. 


Uncensuring is absolutely crucial when objectivity is the goal. 


Uncensored models also “reason” a bit better. As do humans.


The Q8 quants can be run with under 16GB of RAM and the Q4 with 10, on vanilla LLAMA CPP, if you keep the context length within reason.

 

As search library, I tried a few, but finally stuck to DuckDuckGo search, as it requires no API key, while supporting a number of engines (theoretically 9 for text, but Google does not work and Wikipedia and Grokipedia fail more often than not, but that still leaves you with six).


Both the search and the inference are blazingly fast on a venerable RTX 3090. 


So the chatbot is now actually useful.
