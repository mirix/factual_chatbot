COMPONENTS
The model itself is Ministral-3-14B-Instruct. European and pretty solid for the size. The reasoning version takes too long.
I used a version uncensored with the Absolute Heresy protocol. Uncensoring is absolutely crucial when objectivity is the goal. Uncensored models also “reason” a bit better. As do humans.
The Q8 quants can be run with under 16GB of RAM and the with 10, with vanilla LLAMA CPP, if you keep the context length within reason. 
As search library, I tried a few, but finally stuck to DuckDuckGo search, as it requires no API key, while supporting a number of engines (theoretically 9 for text, but Google does not work and Wikipedia and Grokipedia fail more often than not, but that still leaves you with six).
